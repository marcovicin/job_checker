{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746949959156,
     "user": {
      "displayName": "Marco Vicini",
      "userId": "14649293531632007302"
     },
     "user_tz": -120
    },
    "id": "21iwCirf9ySl"
   },
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Utcw2L3Ipaq"
   },
   "source": [
    "First, mount google drive and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22374,
     "status": "ok",
     "timestamp": 1746949981534,
     "user": {
      "displayName": "Marco Vicini",
      "userId": "14649293531632007302"
     },
     "user_tz": -120
    },
    "id": "TIBWtwyD7DhV",
    "outputId": "aefd5fd5-3cde-4080-affc-6761edb10efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# First, mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import shutil\n",
    "import re\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTjTVJPc7dRQ"
   },
   "source": [
    "Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1746949981899,
     "user": {
      "displayName": "Marco Vicini",
      "userId": "14649293531632007302"
     },
     "user_tz": -120
    },
    "id": "9FVmNcKqNPE6"
   },
   "outputs": [],
   "source": [
    "# Path settings\n",
    "BASE_FOLDER = \"/content/drive/MyDrive/Colab_Notebooks/job_checker\" # Change this to your desired path\n",
    "STORAGE_FOLDER = f\"{BASE_FOLDER}/career_pages\"\n",
    "ARCHIVE_FOLDER = f\"{STORAGE_FOLDER}/archive\"\n",
    "\n",
    "# Ensure all folders exist\n",
    "for folder in [STORAGE_FOLDER, ARCHIVE_FOLDER]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"Created directory: {folder}\")\n",
    "\n",
    "GSHEET_CSV_URL = \"https://docs.google.com/spreadsheets/d/ALPHANUMERIC CODE OF YOUR GSHEET /gviz/tq?tqx=out:csv&sheet=NAME OF YOUR TAB\"\n",
    "YOUR_EMAIL = \"YOUR EMAIL\"\n",
    "YOUR_EMAIL_PASSWORD = \"APP PASSWORD OF YOUR EMAIL (NOT YOUR NORMAL EMAIL PASSWORD)\"\n",
    "SEND_TO_EMAIL = \"YOUR EMAIL\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50396,
     "status": "ok",
     "timestamp": 1746950032298,
     "user": {
      "displayName": "Marco Vicini",
      "userId": "14649293531632007302"
     },
     "user_tz": -120
    },
    "id": "5rAZUIgENn9p",
    "outputId": "d916037b-fec5-4e11-e202-e8d4b312960f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Checking: https://jobs.orbisk.com/vacancy\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://eden-projects.rippling-ats.com/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://careers.southpole.com/jobs\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://nadara.wd3.myworkdayjobs.com/External\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.solarpowereurope.org/about/careers\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://eit-culture-creativity.eu/about-us/career-opportunities/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://careers.landlifecompany.com/?_gl=1*gwgcls*_gcl_au*OTM1NDQ4MzU4LjE3NDU5Mjg4NzA.\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://recruiting.ultipro.com/RAI1015FORES/JobBoard/7a1c3d86-f0fa-4e0e-a501-dcfedd4f7d8c/?q=&o=postedDateDesc\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://ats.rippling.com/en-GB/trees/jobs?searchQuery=&workplaceType=&country=&state=&city=&page=0\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://everwave.de/en/team/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://careers.samara.energy/jobs?location_id=690304\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://climatekic.recruitee.com/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://europe.wetlands.org/about-us/vacancies/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.eufic.org/en/who-we-are/work-at-eufic\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://buildingdecarb.org/about-us/careers\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://jobs.lever.co/repurposeglobal\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.terraformation.com/careers\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.carbon-direct.com/careers#open-roles\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://theoceancleanup.com/careers/#jobs\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://careers.icmpd.org/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.responsability.com/en/about/careers\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://fairfood.org/en/about-us/vacancies/\n",
      "✅ Change detected!\n",
      "\n",
      " Checking: https://www.efsolareitalia.com/lavora-con-noi/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://weworld.intervieweb.it/en/career\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://earthrights.org/careers/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.woah.org/en/career/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://jobs.lever.co/generation\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.nutritionintl.org/careers/job-seekers/\n",
      "✅ Change detected!\n",
      "\n",
      " Checking: https://peacebrigades.org/en/get-involved/jobs-pbi\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://phg.tbe.taleo.net/phg02/ats/careers/v2/searchResults?org=CAREUSA&cws=52\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://careers.amnesty.org/jobs/vacancy/find/results/\n",
      "✅ Change detected!\n",
      "\n",
      " Checking: https://www.hollows.org/careers/\n",
      "✅ Change detected!\n",
      "\n",
      " Checking: https://www.acted.org/en/get-involved/join-us/vacancies/\n",
      "🤷‍♀️ No change.\n",
      "\n",
      " Checking: https://www.handinhandinternational.org/join-the-team/#jobposting\n",
      "🤷‍♀️ No change.\n",
      "[EMAIL SENT] 🚀\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_domain_from_url(url):\n",
    "    \"\"\"Extract a readable domain name from URL, limited to 25 chars\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc.replace(\"www.\", \"\")\n",
    "    # Remove non-alphanumeric characters\n",
    "    domain = re.sub(r'[^a-zA-Z0-9]', '_', domain)\n",
    "    # Limit to 25 characters max\n",
    "    if len(domain) > 25:\n",
    "        domain = domain[:25]\n",
    "    return domain\n",
    "\n",
    "\n",
    "#This function creates unique, consistent filenames regardless of how complex or long the URL is,\n",
    "# it avoids special characters in URL (like '/', '?', '&') and has a Fixed length\n",
    "def get_filename_for_url(url, date_str=None):\n",
    "    \"\"\"Generate filename with date tag and readable domain\"\"\"\n",
    "    if date_str is None:\n",
    "        date_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    domain = get_domain_from_url(url)\n",
    "    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]  # Use shorter hash\n",
    "\n",
    "    return os.path.join(STORAGE_FOLDER, f\"{domain}_{url_hash}_{date_str}.txt\")\n",
    "\n",
    "\n",
    "\n",
    "def get_yesterday_file(url):\n",
    "    \"\"\"Find yesterday's file for a given URL\"\"\"\n",
    "    yesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    domain = get_domain_from_url(url)\n",
    "    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n",
    "\n",
    "    # Pattern to find yesterday's file\n",
    "    pattern = f\"{domain}_{url_hash}_{yesterday}.txt\"\n",
    "\n",
    "    # Look for exact match\n",
    "    yesterday_file = os.path.join(STORAGE_FOLDER, pattern)\n",
    "    if os.path.exists(yesterday_file):\n",
    "        return yesterday_file\n",
    "\n",
    "    # If not found, look for any file with similar pattern from previous days\n",
    "    files = os.listdir(STORAGE_FOLDER)\n",
    "    pattern_base = f\"{domain}_{url_hash}_\"\n",
    "\n",
    "    # Filter files matching our URL and sort by date (newest first)\n",
    "    matching_files = [f for f in files if f.startswith(pattern_base) and f.endswith(\".txt\")]\n",
    "    matching_files.sort(reverse=True)\n",
    "\n",
    "    # Return the most recent file if any found\n",
    "    if matching_files:\n",
    "        return os.path.join(STORAGE_FOLDER, matching_files[0])\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def archive_old_files(days_threshold=4):\n",
    "    \"\"\"Move files older than specified days to archive folder\"\"\"\n",
    "    today = datetime.datetime.now()\n",
    "    files = os.listdir(STORAGE_FOLDER)\n",
    "\n",
    "    # Only process txt files\n",
    "    txt_files = [f for f in files if f.endswith(\".txt\")]\n",
    "\n",
    "    archived_count = 0\n",
    "    for filename in txt_files:\n",
    "        # Extract date from filename (assumes format ending with YYYY-MM-DD.txt)\n",
    "        try:\n",
    "            date_str = filename.split(\"_\")[-1].replace(\".txt\", \"\")\n",
    "            file_date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "            # Check if file is older than threshold\n",
    "            days_old = (today - file_date).days\n",
    "            if days_old >= days_threshold:\n",
    "                src_path = os.path.join(STORAGE_FOLDER, filename)\n",
    "                dst_path = os.path.join(ARCHIVE_FOLDER, filename)\n",
    "                shutil.move(src_path, dst_path)\n",
    "                archived_count += 1\n",
    "        except (ValueError, IndexError):\n",
    "            # Skip files that don't match our naming pattern\n",
    "            continue\n",
    "\n",
    "    if archived_count > 0:\n",
    "        print(f\"Archived {archived_count} files older than {days_threshold} days\")\n",
    "\n",
    "\n",
    "def load_urls_from_gsheet_csv(gsheet_csv_url):\n",
    "    try:\n",
    "        df = pd.read_csv(gsheet_csv_url)\n",
    "        urls = df.iloc[:, 0].dropna().tolist()  # Only column A\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not load data from Google Sheets: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def fetch_page_text(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, timeout=10, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def send_email(subject, body, is_html=False):\n",
    "    try:\n",
    "        if is_html:\n",
    "            msg = MIMEText(body, 'html')\n",
    "        else:\n",
    "            msg = MIMEText(body)\n",
    "\n",
    "        msg[\"Subject\"] = subject\n",
    "        msg[\"From\"] = YOUR_EMAIL\n",
    "        msg[\"To\"] = SEND_TO_EMAIL\n",
    "\n",
    "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n",
    "            server.login(YOUR_EMAIL, YOUR_EMAIL_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        print(\"[EMAIL SENT] 🚀\")\n",
    "    except Exception as e:\n",
    "        print(f\"[EMAIL ERROR] {e}\")\n",
    "\n",
    "def main():\n",
    "    # First, archive old files\n",
    "    archive_old_files(days_threshold=4)\n",
    "\n",
    "    # Then load URLs and check for changes\n",
    "    CAREER_URLS = load_urls_from_gsheet_csv(GSHEET_CSV_URL)\n",
    "    if not CAREER_URLS:\n",
    "        print(\"[FATAL] No URLs loaded from Google Sheet. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Track changes for email report\n",
    "    changed_urls = []\n",
    "\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    for url in CAREER_URLS:\n",
    "        print(f\"\\n Checking: {url}\")\n",
    "        new_content = fetch_page_text(url)\n",
    "        if new_content is None:\n",
    "            continue\n",
    "\n",
    "        # Get today's filename\n",
    "        current_filename = get_filename_for_url(url, today)\n",
    "\n",
    "        # Find yesterday's or most recent previous file\n",
    "        previous_file = get_yesterday_file(url)\n",
    "\n",
    "        if previous_file and os.path.exists(previous_file):\n",
    "            with open(previous_file, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                old_content = f.read()\n",
    "\n",
    "            if new_content != old_content:\n",
    "                print(\"✅ Change detected!\")\n",
    "                changed_urls.append(url)\n",
    "\n",
    "                # Save new content\n",
    "                with open(current_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(new_content)\n",
    "            else:\n",
    "                print(\"🤷‍♀️ No change.\")\n",
    "        else:\n",
    "            print(\"First-time check or no recent file found.\")\n",
    "            changed_urls.append(url)\n",
    "\n",
    "            # Save initial content\n",
    "            with open(current_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(new_content)\n",
    "\n",
    "    # Send email report\n",
    "    if changed_urls:\n",
    "        # Create super simple email with just the URLs\n",
    "        body = \"These career pages have updates:\\n\\n\"\n",
    "        for url in changed_urls:\n",
    "            body += f\"{url}\\n\"\n",
    "\n",
    "        body += \"\\n\\nCopy the list in gsheet and open them all at once\"\n",
    "\n",
    "        subject = f\"🌸 {len(changed_urls)} Career Page{'s' if len(changed_urls) > 1 else ''} Updated!\"\n",
    "        send_email(subject=subject, body=body)\n",
    "    else:\n",
    "        body = \"Nothing changed\"\n",
    "        subject = \"All the same \"\n",
    "        send_email(subject=subject, body=body)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWV2EXlv7hLo"
   },
   "source": [
    "Functions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1Hl0UZzIuXstfiOEVwxCd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
